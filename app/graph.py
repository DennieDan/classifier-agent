import logging
import operator
import time
from typing import Annotated, List, Literal, TypedDict, Union

from constants import get_llm
from index_query import index_query
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langgraph.graph import END, START, StateGraph
from prompts.search_prompt import RECURSIVE_SEARCH_PROMPT, SEARCH_PROMPT
from prompts.supervisor_prompt import SUPERVISOR_PROMPT
from pydantic import BaseModel, Field

# Configure logging to log.txt
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("graph_log.txt"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)

from langgraph.graph import END, StateGraph


class AgentState(TypedDict):
    # The list of messages in the conversation
    messages: Annotated[List[BaseMessage], operator.add]
    # The item to search for (extracted by supervisor from user input)
    search_item: str
    # The final answer generated by the worker
    final_answer: str
    # The confidence score assigned by the supervisor
    confidence: float
    # Feedback for the worker if confidence is low (e.g. "search recursively")
    feedback: str
    # Whether we are in recursive search mode (retry after low confidence)
    recursive_search: bool
    # Supervisor's routing decision
    next_action: str


class SupervisorDecision(BaseModel):
    search_item: str = Field(
        description="The specific item/product name extracted from user input that needs its HS-Code searched."
    )
    next_action: Literal["search_agent", "FINISH"] = Field(
        description="Select 'search_agent' to find/refine an answer, or 'FINISH' if the answer is satisfactory."
    )
    confidence_score: float = Field(
        description="A score between 0.0 and 1.0 representing confidence in the current answer. Use 0.0 when no search has been done yet."
    )
    feedback: str = Field(
        description="If confidence is low, provide specific instructions to search recursively (e.g. try broader terms, synonyms, related categories)."
    )


llm = get_llm()
# llm = ChatOpenAI(model="gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))


def supervisor_node(state: AgentState):
    """
    Supervisor Agent: Extracts search item from input, then reviews search results and decides next steps.
    """
    messages = state["messages"]

    system_prompt = SUPERVISOR_PROMPT

    structured_llm = llm.with_structured_output(SupervisorDecision)

    response = structured_llm.invoke([SystemMessage(content=system_prompt)] + messages)

    print(
        f"\n--- [Supervisor] Search item: {response.search_item} | Confidence: {response.confidence_score*100}% | Decision: {response.next_action} ---"
    )

    return {
        "search_item": response.search_item,
        "confidence": response.confidence_score,
        "feedback": response.feedback,
        "next_action": response.next_action,
        "recursive_search": response.next_action == "search_agent"
        and bool(state.get("final_answer")),
    }


def search_agent_node(state: AgentState):
    """
    Search Agent: Searches for HS-Code of the extracted item.
    Uses recursive search prompt when confidence was low (< 0.9).
    In a real app, this would call a VectorStore or RAG pipeline.
    """
    search_item = state.get("search_item", "")
    feedback = state.get("feedback", "")
    recursive_search = state.get("recursive_search", False)

    if recursive_search and feedback:
        # Use recursive search prompt when retrying after low confidence
        prompt = RECURSIVE_SEARCH_PROMPT.format(
            search_item=search_item,
            feedback=feedback,
        )
        print(f"\n--- [Search Agent] Recursive search for: {search_item} ---")
    else:
        prompt = SEARCH_PROMPT.format(search_item=search_item)
        print(f"\n--- [Search Agent] Searching for: {search_item} ---")

    # TODO: Integrate with VectorStore/RAG (e.g. index_query)
    # For now, simulate: first pass returns uncertain result; recursive pass returns better result
    # if recursive_search and feedback:
    #     answer = f"Based on recursive search with feedback '{feedback[:50]}...': HS-Code 8418.90 (Parts of furnaces, radiators) applies to radiator panels."
    # else:
    #     answer = f"I found some general info about '{search_item}', but I am not certain about the specific HS-Code."

    logger.info(f"\n--- [Search Agent] Searching for: {search_item} ---")

    answer = index_query(prompt)

    logger.info(f"\n--- [Search Agent] Final Answer: {answer} ---")
    return {
        "messages": [HumanMessage(content=answer, name="search_agent")],
        "final_answer": answer,
    }


def route_logic(state: AgentState) -> str:
    # Use supervisor's next_action: FINISH if confidence > 0.9, else search_agent (recursive)
    return state.get("next_action", "search_agent")


# Build the Graph
workflow = StateGraph(AgentState)

# Nodes
workflow.add_node("supervisor", supervisor_node)
workflow.add_node("search_agent", search_agent_node)

# Edges
# Start -> supervisor (supervisor receives input, extracts search_item)
workflow.add_edge(START, "supervisor")

# search_agent -> supervisor (supervisor reviews result, checks confidence)
workflow.add_edge("search_agent", "supervisor")

# supervisor -> Conditional (search_agent or FINISH; if confidence < 0.9, loop to search_agent with recursive prompt)
workflow.add_conditional_edges(
    "supervisor", route_logic, {"search_agent": "search_agent", "FINISH": END}
)

app = workflow.compile()


if __name__ == "__main__":
    initial_state = {
        "messages": [HumanMessage(content="What is the HS-Code of Radiator panels?")],
        "search_item": "",
        "final_answer": "",
        "confidence": 0.0,
        "feedback": "",
        "recursive_search": False,
        "next_action": "",
    }

    # Execute the graph
    start_time = time.time()
    for output in app.stream(initial_state):
        pass  # The print statements in nodes will show progress
    end_time = time.time()
    logger.info(f"Total time taken: {end_time - start_time} seconds")