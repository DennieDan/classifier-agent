import asyncio
import json
import logging
import operator
import os
import time
from typing import Annotated, Dict, List, Literal, TypedDict, Union

# Resolve paths so MCP stdio servers can be found regardless of process cwd
_GRAPH_DIR = os.path.dirname(os.path.abspath(__file__))
_TOOLS_DIR = os.path.join(_GRAPH_DIR, "tools")

from index_query import IndexQuery
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.graph import END, START, StateGraph
from prompts.search_prompt import SEARCH_PROMPT
from prompts.supervisor_prompt import SUPERVISOR_PROMPT
from pydantic import BaseModel, Field

# Configure logging (set to logging.WARNING to turn off info logs)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("graph_log.txt"),
        logging.StreamHandler(),
    ],
)
# logging.disable(logging.INFO)
logger = logging.getLogger(__name__)


# --- 1. Define State ---
class AgentState(TypedDict):
    # The list of messages in the conversation
    messages: Annotated[List[BaseMessage], operator.add]
    # The rules retrieved from the STCCED 2022
    rules: str
    # Supervisor's routing decision
    next_action: str
    # The final answer generated by the worker
    final_answer: str
    # The confidence score assigned by the supervisor
    confidence: float
    # Reasoning for the decision from Supervisor
    supervisor_reasoning: str
    # Instructions for the search agent
    instructions: str
    # Search results from the search agent
    search_results: str
    # Previous search results from the search agent
    previous_search_results: str


# --- 2. Define Structured Output ---
class SupervisorDecision(BaseModel):
    next_action: Literal["search_agent", "FINISH"] = Field(
        description="Select 'search_agent' to find/refine an answer, or 'FINISH' if the answer is satisfactory (>90% confidence)."
    )
    confidence: float = Field(
        description="A score between 0.0 and 1.0 representing confidence. 1.0 = Certain."
    )
    feedback: str = Field(
        description="Reasoning for the decision. If confidence is low, provide specific instructions to search recursively (e.g. 'Check Note 3 of Chapter 85')."
    )
    reasoning_step: str = Field(
        description="Reasoning for the decision. If confidence is low, provide specific instructions to search recursively (e.g. 'Check Note 3 of Chapter 85')."
    )
    best_search_result: str = Field(
        description="The best search result from the search agent"
    )


class SearchQuery(BaseModel):
    """Query to run against the index. Use supervisor instructions and user input to form a precise query."""

    query: str = Field(
        description="The search query to run against the classification database (e.g. HS code for a product)."
    )


class SearchAgentDecision(BaseModel):
    search_results: str = Field(
        description="Possible responses from the search agent and reasoning for each possible response"
    )


# --- 3. Define Nodes ---


class AgentGraph:
    def __init__(self, llm: BaseChatModel):
        self.llm = llm
        self.index_query = IndexQuery()
        self.workflow = StateGraph(AgentState)

        self.server_params = {
            "regulatory_server": {
                "command": "python",
                "args": [os.path.join(_TOOLS_DIR, "regulatory_server.py")],
                "transport": "stdio",
                "cwd": _GRAPH_DIR,  # so server can import index_query, etc.
            },
            "agent_tools": {
                "command": "python",
                "args": [os.path.join(_TOOLS_DIR, "agent_tools.py")],
                "transport": "stdio",
                "cwd": _GRAPH_DIR,
            },
        }
        self.client = MultiServerMCPClient(self.server_params)
        # Add Nodes
        self.workflow.add_node("supervisor", self.supervisor_node)
        self.workflow.add_node("search_agent", self.search_agent_node)
        # Add Edges
        # 3. Supervisor -> Decision (Search or Finish)
        self.workflow.add_conditional_edges(
            "supervisor",
            self.route_logic,
            {"search_agent": "search_agent", "FINISH": END, "supervisor": "supervisor"},
        )

        # 4. Search Agent -> Supervisor (Loop back for validation)
        self.workflow.add_edge(START, "supervisor")
        self.workflow.add_edge("search_agent", "supervisor")

        self.app = self.workflow.compile()

    async def supervisor_node(self, state: AgentState):
        """
        Supervisor Agent: analyzing input + rules + previous search results.
        """
        messages = state["messages"]
        search_results = state.get("search_results", "")
        current_rules = state.get("rules", "")

        # --- Step 1: MCP â€” load all tools (regulatory_server + agent_tools) for context and optional rule fetch ---
        all_tools: List = []
        try:
            all_tools = await self.client.get_tools()  # both servers
        except BaseException as e:
            logger.warning(
                "MCP connection failed (check that server scripts run from app/tools with cwd=app): %s",
                e,
            )

        # Build agent context: list of tool names and descriptions for the prompt
        available_tools_text = "No tools loaded (MCP unavailable)."
        if all_tools:
            try:
                lines = [
                    f"- **{t.name}**: {(t.description or '(No description)').strip()}"
                    for t in all_tools
                ]
                available_tools_text = "\n".join(lines)
            except Exception as e:
                logger.warning(f"Could not format MCP tool list: {e}")

        # Fetch GIR rules via MCP if we don't have them yet
        if not current_rules:
            try:
                get_rules_tool = next(
                    (t for t in all_tools if t.name == "get_regulatory_rules"), None
                )
                if get_rules_tool:
                    result = await get_rules_tool.ainvoke({"rule_number": "all"})
                    current_rules = (
                        result
                        if isinstance(result, str)
                        else getattr(result, "content", str(result))
                    )
                    logger.info("--- [Supervisor] Fetched GIR Rules via MCP ---")
                else:
                    current_rules = "Error: get_regulatory_rules tool not found."
            except Exception as e:
                logger.error(f"Failed to fetch rules via MCP: {e}")
                current_rules = "Error retrieving rules. Proceed with caution."

        # --- Step 2: Prompt Engineering ---
        tools_context = f"Latest Search Agent Output: {search_results}"
        rules_context = f"Official STCCED 2022 Rules:\n{current_rules[:3000]}..."  # Truncate if too long
        dynamic_system_prompt = SUPERVISOR_PROMPT.format(
            user_input=messages[0].content,
            available_tools=available_tools_text,
            tools_context=tools_context,
            rules_context=rules_context,
        )

        system_msg = SystemMessage(content=dynamic_system_prompt)
        conversation: List[BaseMessage] = [system_msg] + list(messages)

        # --- Step 3: Tool-calling loop (LLM can invoke MCP tools) ---
        llm_with_tools = self.llm.bind_tools(all_tools)
        response = await llm_with_tools.ainvoke(
            [SystemMessage(content=dynamic_system_prompt)]
        )
        # --- Step 4: Final structured decision (with full context including tool results) ---
        # structured_llm = self.llm.with_structured_output(SupervisorDecision)
        # response = await structured_llm.ainvoke(conversation)

        logger.info(f"--- [Supervisor] Response: {response} ---")

        response = SupervisorDecision.model_validate(json.loads(response.content))

        logger.info(
            f"\n--- [Supervisor] Decision: {response.next_action} | Confidence: {response.confidence*100}% ---"
        )
        logger.info(f"--- [Supervisor] Reasoning: {response.reasoning_step} ---")
        logger.info(f"--- [Supervisor] Feedback: {response.feedback} ---")

        # --- Step 4: State Update ---
        if response.next_action == "FINISH":
            return {
                "next_action": "FINISH",
                "supervisor_reasoning": response.reasoning_step,
                "confidence": response.confidence,
                "final_answer": response.feedback,  # The Auditor's Log
                "rules": current_rules,  # Persist rules in state
            }

        return {
            "instructions": response.feedback,  # Strategy for the worker
            "next_action": response.next_action,
            "rules": current_rules,  # Persist rules so we don't fetch them every time
            "previous_search_results": search_results,
        }

    def search_agent_node(self, state: AgentState):
        """
        Search Agent: Uses index_query as a tool to fetch from the index, then formats the result as SearchAgentDecision.
        """
        messages = state["messages"]
        instructions = state.get("instructions", "")
        previous_search_results = state.get("previous_search_results", "")

        prompt = SEARCH_PROMPT.format(
            user_input=messages[0].content,
            previous_search_results=previous_search_results,
            supervisor_instructions=instructions,
        )

        retrieval_result = self.index_query.index_query(query=prompt)
        logger.info(f"--- [Search Agent] Retrieval Result: {retrieval_result} ---")

        return {
            "search_results": retrieval_result,
            "next_action": "supervisor",
        }

    def route_logic(self, state: AgentState) -> str:
        return state.get("next_action", "search_agent")
